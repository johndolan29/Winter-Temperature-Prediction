[
  {
    "objectID": "Winter Temperature Prediction.html",
    "href": "Winter Temperature Prediction.html",
    "title": "",
    "section": "",
    "text": "Method 2: Machine learning\nLet’s see if we can use ML to perform a better prediction. First, we need to split the data up into training and test sets.\n\n#### ML training with single month\n\ndef dataframe_to_training_and_test_sets(df, pct_split, months_used_for_prediction, label_col):\n    \n    autumn = [x in months_used_for_prediction for x in df.index.month]\n    df_filtered = df[autumn]\n    \n    df_train = df_filtered.loc[df_filtered.index[:int(len(df_filtered.index)*pct_split)],:]\n    df_test = df_filtered.loc[df_filtered.index[int(len(df_filtered.index)*pct_split):],:]\n    \n\n    y_train = df_train[label_col].round(3).values\n    X_train = df_train.drop(columns = [label_col]).values\n\n    y_test = df_test[label_col].round(3).values\n    X_test = df_test.drop(columns = [label_col]).values\n    return X_train,y_train, X_test, y_test, df_train, df_test\n\nlabel_col = 'Winter Mean Temperature'\nall_months_with_winter_means['Month'] = all_months_with_winter_means.index.month\nall_months_with_winter_means_shuffled = all_months_with_winter_means.sample(frac=1, random_state=123) # going to shuffle the data so that we have some of the more recent, warmer years in the training set\nX_train, y_train, X_test, y_test, df_train, df_test = dataframe_to_training_and_test_sets(all_months_with_winter_means_shuffled, 0.8, [9,10,11], label_col)\n\nThen we need to find a suitable regression model to perform our predictions:\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import model_selection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, RepeatedKFold\n\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport pandas as pd\ndef run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n    '''\n    Lightweight script to test many models and find winners\n    :param X_train: training split\n    :param y_train: training target vector\n    :param X_test: test split\n    :param y_test: test target vector\n    :return: DataFrame of predictions\n    '''\n    \n    dfs = []\n    models = [\n              ('LR', LinearRegression()), \n              ('SVR', SVR()),\n              ('RFR', RandomForestRegressor()),\n              ('GBR', GradientBoostingRegressor()),\n                ('XGR', XGBRegressor()),\n            ]\n    results = []\n    names = []\n    scoring = ['r2']\n    for name, model in models:\n            kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=123)\n            cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n            clf = model.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            print(name)\n            print(\"R^2: \",r2_score(y_test, y_pred))\n            results.append(cv_results)\n            names.append(name)\n    return \n\nrun_exps(X_train, y_train, X_test, y_test)\n\nLR\nR^2:  0.1603952612925963\nSVR\nR^2:  -0.007026424218675276\nRFR\nR^2:  0.39754923181773816\nGBR\nR^2:  0.4308514206317199\nXGR\nR^2:  0.319008600725606\n\n\nIt looks like Linear Regression is the model with the best r^2\n\ndef naive_confidence_intervals(model, X, y, alpha):\n    residuals = y - model.predict(X)\n    #plt.hist(residuals) # histogram indicates the errors are not normally distributed, therefore need to use the jackknife method\n    ci = np.quantile(residuals, 1 - alpha)\n    return ci\n\ndef jackknife_confidence_intervals(model, X_train, y_train, X_test, y_test, alpha):\n    kf = KFold(n_splits=len(y_train)-1, shuffle=True, random_state=123)\n    res = []\n    for train_index, test_index in kf.split(X_train):\n        X_train_, X_test_ = X_train[train_index], X_train[test_index]\n        y_train_, y_test_ = y_train[train_index], y_train[test_index]\n\n        model.fit(X_train_, y_train_)\n        res.extend(list(y_test_ - model.predict(X_test_)))\n    ci = np.quantile(res, 1 - alpha)\n    return ci\n\ndef train_and_evaluate(model_type, X_train, y_train, X_test, y_test):\n    model = model_type\n    model.fit(X_train,y_train)\n    # define model evaluation method\n    \n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n    scores = cross_val_score(model, X_train,y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scores = np.abs(scores)\n    print('Mean MAE: %.5f (%.8f)' % (scores.mean(), scores.std()) )\n\n    mae = mean_absolute_error(y_test, model.predict(X_test))\n    mape = mean_absolute_percentage_error(y_test, model.predict(X_test))\n    \n    ci = jackknife_confidence_intervals(model, X_train, y_train, X_test, y_test, 0.2) # CI at the 80th percentile\n    #ci = naive_confidence_intervals(model,X_test, y_test, 0.05)\n    \n    r2 = r2_score(y_test, model.predict(X_test))\n    print(\"r^2 (test set): \", r2)\n\n    \n    print(\"MAE (test set): \",mae)\n    return model, ci\n\ndef scatter_plot_of_results(y_test, predicted_vals, interval):\n    plt.scatter(y_test, predicted_vals, label='Predictions')\n    plt.errorbar(y_test, predicted_vals, yerr=interval, fmt='o', label = \"CI at 80th pctile\")\n    plt.plot(y_test, y_test, label='Perfect prediction line', c='r')\n    plt.xlabel('Actual ($^\\circ$C)')\n    plt.ylabel('Predicted ($^\\circ$C)')\n    plt.title('Actual vs predicted')\n    plt.legend()\n    return\n    \n\n\nmodel, interval = train_and_evaluate(RandomForestRegressor(), X_train, y_train, X_test, y_test)\nscatter_plot_of_results(y_test, model.predict(X_test), interval)\n\nMean MAE: 1.17752 (0.31619627)\nr^2 (test set):  0.6595337373342609\nMAE (test set):  1.1167307692307697\n\n\n\n\n\n\ndef get_forecast(percentile):\n    forecasts = {0: 'Very Cold', 20: 'Colder than average', 40: 'Around Average', 60: 'Milder than average', 80: 'Very Mild',100: \"\"}\n    keys = list(forecasts.keys())\n    predicted_forecast = forecasts[[keys[k] for k in range(len(keys)) if percentile > keys[k] and  percentile < keys[k+1]][0]]\n    return predicted_forecast\n\ndef winter_prediction(percentile, date):\n    print(\"Prediction: this winter (as predicted with the latest data from {date} \\nwill be in the {p}th percentile of all UK winters from the dataset\\n\".format(date = predicted_from_date, p = round(percentile,2)))\n\nfeatures_df['Month'] = features_df.index.month\npredicted_from_date = features_df.index[-1]\npredict_this_winter = model.predict(np.array(features_df.loc[predicted_from_date].values).reshape(1,-1))[0]\npredict_this_winter\n\n4.83075\n\n\n\npercentile = stats.percentileofscore(all_winter_means.values, predict_this_winter)\nwinter_prediction(percentile, predicted_from_date)\nget_forecast(percentile)\n\nPrediction: this winter (as predicted with the latest data from 2022-09-01 00:00:00 \nwill be in the 56.82th percentile of all UK winters from the dataset\n\n\n\n'Around Average'\n\n\nOverall, this method does much better than the analogue method we used before, although it could be improved. Can we do better than this?\n\n\n\nMethod 3: ML with a lookback\nThis method is similar to method 2, however, we are going to add a lookback to the features, i.e. not only taking into account the current months data as a feature, but also previous months.\n\n#### ML training with many months\n\ndef dataframe_with_lookback(data, n_in=1, n_out=1, dropnan=True):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [(df.columns[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [(df.columns[j]) for j in range(n_vars)]\n        else:\n            names += [(df.columns[j]+'(t+%d)' % (i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n\nTo give the model the best chance of making a good prediction, let’s use the data for November (as the latest data), and lookback 2 months.\n\ndf_shifted = dataframe_with_lookback(all_months_with_winter_means, n_in=2, n_out=1, dropnan=True)\ndf_shifted['Month'] = df_shifted.index.month\n\ndf_shifted = df_shifted.drop(columns = [col for col in df_shifted.columns if label_col in col and col != label_col])\n\ndf_shifted = df_shifted.sample(frac=1.0, random_state=123)\nX_train, y_train, X_test, y_test, df_train, df_test = dataframe_to_training_and_test_sets(df_shifted, 0.8, [9,10,11], label_col)\n\n\nrun_exps(X_train, y_train, X_test, y_test)\n\nLR\nR^2:  -0.10098019977034256\nSVR\nR^2:  -0.21862449182580934\nRFR\nR^2:  0.1740263054356963\nGBR\nR^2:  0.35616737999262116\nXGR\nR^2:  0.1584880354841618\n\n\nXGBRegressor seems to be the best performer, albeit pretty poor performance\n\nmodel, interval = train_and_evaluate(XGBRegressor(), X_train, y_train, X_test, y_test)\nscatter_plot_of_results(y_test, model.predict(X_test), interval)\n\nMean MAE: 0.90292 (0.13446777)\nr^2 (test set):  0.18066621573589392\nMAE (test set):  0.8392910396869365\n\n\n\n\n\n\npredicted_from_date = df_shifted.index[-1]\nfeatures = df_shifted.drop(columns=['Winter Mean Temperature']).loc[predicted_from_date]\n\npredict_this_winter = model.predict(features.values.reshape(1,-1))[0]\n\npercentile = stats.percentileofscore(all_winter_means.values, predict_this_winter)\nwinter_prediction(percentile, predicted_from_date)\nget_forecast(percentile)\n\nPrediction: this winter (as predicted with the latest data from 2021-09-01 00:00:00 \nwill be in the 36.36th percentile of all UK winters from the dataset\n\n\n\n'Colder than average'\n\n\nHmm, the model does not seemed to have improved with more features. Perhaps there are some redundant features, let’s rank the features:\n\n######## Feature Importance Section\n\n# define the method\nrfe = RFE(estimator=XGBRegressor(), n_features_to_select=5)\nrfe.fit(X_train,y_train)\n\ndf_feature_selection = pd.DataFrame([], columns = ['Feature Name', 'Ranking'])\nfor i in range(X_train.shape[1]):\n    #print('Column: %d, Name: %s,  Selected %s, Rank: %.3f' % (i,df_train.drop(columns=['Winter Mean Temperature']).columns[i],  rfe.support_[i], rfe.ranking_[i]))\n    df_feature_selection = df_feature_selection.append({'Feature Name': df_train.drop(columns=['Winter Mean Temperature']).columns[i], 'Ranking': rfe.ranking_[i]}, ignore_index=True)\n\ndf_feature_selection.sort_values('Ranking')\n\n\n\n\n\n  \n    \n      \n      Feature Name\n      Ranking\n    \n  \n  \n    \n      36\n      UK_rainfall\n      1\n    \n    \n      32\n      AMO\n      1\n    \n    \n      17\n      Sunspots(t-1)\n      1\n    \n    \n      15\n      Nino3.4(t-1)\n      1\n    \n    \n      14\n      SOI(t-1)\n      1\n    \n    \n      35\n      PNA\n      2\n    \n    \n      4\n      Sunspots(t-2)\n      3\n    \n    \n      16\n      PDO(t-1)\n      4\n    \n    \n      7\n      AO(t-2)\n      5\n    \n    \n      30\n      Sunspots\n      6\n    \n    \n      6\n      AMO(t-2)\n      7\n    \n    \n      3\n      PDO(t-2)\n      8\n    \n    \n      2\n      Nino3.4(t-2)\n      9\n    \n    \n      27\n      SOI\n      10\n    \n    \n      26\n      Eurasian_Snow_Cover\n      11\n    \n    \n      33\n      AO\n      12\n    \n    \n      13\n      Eurasian_Snow_Cover(t-1)\n      13\n    \n    \n      10\n      UK_rainfall(t-2)\n      14\n    \n    \n      9\n      PNA(t-2)\n      15\n    \n    \n      8\n      NAO(t-2)\n      16\n    \n    \n      28\n      Nino3.4\n      17\n    \n    \n      19\n      AMO(t-1)\n      18\n    \n    \n      22\n      PNA(t-1)\n      19\n    \n    \n      11\n      UK_mean_temp(t-2)\n      20\n    \n    \n      23\n      UK_rainfall(t-1)\n      21\n    \n    \n      31\n      QBO\n      22\n    \n    \n      0\n      Eurasian_Snow_Cover(t-2)\n      23\n    \n    \n      20\n      AO(t-1)\n      24\n    \n    \n      29\n      PDO\n      25\n    \n    \n      1\n      SOI(t-2)\n      26\n    \n    \n      5\n      QBO(t-2)\n      27\n    \n    \n      18\n      QBO(t-1)\n      28\n    \n    \n      34\n      NAO\n      29\n    \n    \n      21\n      NAO(t-1)\n      30\n    \n    \n      37\n      UK_mean_temp\n      31\n    \n    \n      24\n      UK_mean_temp(t-1)\n      32\n    \n    \n      12\n      Month(t-2)\n      33\n    \n    \n      25\n      Month(t-1)\n      34\n    \n    \n      38\n      Month\n      35\n    \n  \n\n\n\n\n\n#df_shifted = df_shifted[['UK_rainfall','NAO', 'QBO', 'AO(t-1)', 'AMO','Month', 'Winter Mean Temperature']]\ndf_shifted = df_shifted[['UK_rainfall','Nino3.4(t-1)', 'Sunspots(t-1)', 'SOI(t-1)', 'AMO','Month', 'Winter Mean Temperature']]\nX_train, y_train, X_test, y_test, df_train, df_test = dataframe_to_training_and_test_sets(df_shifted, 0.8, [9,10,11], 'Winter Mean Temperature')\n\n\nmodel, interval = train_and_evaluate(XGBRegressor(), X_train, y_train, X_test, y_test)\nscatter_plot_of_results(y_test, model.predict(X_test), interval)\n\nMean MAE: 0.70440 (0.17346970)\nr^2 (test set):  -0.23807772090063328\nMAE (test set):  1.1042655073312613\n\n\n\n\n\nIt doesn’t seem like filtering the features produces any gains. Lesson learned, more features does not alway equal a better model. Intuitively this makes sense, what matters most when trying to predict weather/climate is the most recent observations, not something that occurs 2 months ago.\n\n\n\nMethod 4: Using ML to predict next months mean temperature\n\ndf_shifted = dataframe_with_lookback(all_months_with_winter_means.drop(columns='Winter Mean Temperature'), n_in=2, n_out=1, dropnan=True)\ndf_shifted['Month'] = df_shifted.index.month\n\n\nn_months_ahead = 1\n\ndf_shifted['UK_mean_temp (t+1)'] = df_shifted['UK_mean_temp']\ndf_shifted['UK_mean_temp (t+1)'] = df_shifted.shift(-n_months_ahead)['UK_mean_temp (t+1)']\ndf_shifted = df_shifted.dropna(axis=0)\n\n\nX_train, y_train, X_test, y_test, df_train, df_test = dataframe_to_training_and_test_sets(df_shifted, 0.8, [9,10,11], 'UK_mean_temp (t+1)')\n\n\nrun_exps(X_train, y_train, X_test, y_test)\n\nLR\nR^2:  0.4640143981370094\nSVR\nR^2:  0.03044869387272242\nRFR\nR^2:  0.6697714309544459\nGBR\nR^2:  0.6785911103425025\nXGR\nR^2:  0.6448591987840867\n\n\n\nmodel, interval = train_and_evaluate(XGBRegressor(), X_train, y_train, X_test, y_test)\nscatter_plot_of_results(y_test, model.predict(X_test), interval)\n\nMean MAE: 1.27559 (0.34232159)\nr^2 (test set):  0.6194547144969771\nMAE (test set):  1.198695004903353\n\n\n\n\n\n\ndf_shifted2 = dataframe_with_lookback(features_df, n_in=2, n_out=1, dropnan=True)\ndf_shifted2['Month'] = df_shifted2.index.month\n\n\npredict_using_month = df_shifted2.index[-1] # get last row of dataset (should be the latest data)\npredict_next_month = model.predict(df_shifted2[df_shifted2.index == predict_using_month].values.reshape(1,-1))[0]\n\n\nfrom datetime import timedelta\nprint(predict_next_month, \"+/-\", interval)\nmean_UK_temps_for_predicted_month = df_shifted2[df_shifted2.index.month ==predict_month.month+n_months_ahead]['UK_mean_temp']\nmean_UK_temps_for_predicted_month.hist()\nplt.axvline(predict_next_month, c='r')\nplt.axvline(predict_next_month+interval, c='yellow')\nplt.axvline(predict_next_month-interval, c='yellow')\npercentile = stats.percentileofscore(mean_UK_temps_for_predicted_month, predict_next_month)\n\n\nprint(\"Predicted forecast for month of {date} \\n{fc}\".format(date = str((predict_using_month+timedelta(days=31*n_months_ahead)).date())[:-3], fc= get_forecast(percentile)))\n\n10.371333 +/- 1.174963150024414\nPredicted forecast for month of 2022-10 \nAround Average\n\n\n\n\n\n\nUnsurprisingly, predicting the weather is difficult! Although, we can still draw some conclusions from this analysis. 1. Using climate index analogues is not a good way of predicting 2. Adding data from n months ago does not improve model performance 3. It’s much easier to predict next months mean temperature than the average of the next 3 months 4. It’s very difficult to say with any confidence what the UK winter of 2022/2023 is going to be like, we’ll have to wait and see"
  }
]